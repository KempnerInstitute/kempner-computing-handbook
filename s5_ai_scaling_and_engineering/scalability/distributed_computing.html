
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>17.1. Distributed Computing &#8212; Kempner Institute Computing Handbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=e97a2c88" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-QFZEXWFW26"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QFZEXWFW26');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QFZEXWFW26');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 's5_ai_scaling_and_engineering/scalability/distributed_computing';</script>
    <script src="../../_static/custom.js?v=ba1c093d"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="17.2. Intro to Parallel Computing" href="introduction_to_parallel_computing.html" />
    <link rel="prev" title="17. Scalability" href="README.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/Kempner-logo_Full-Color-Full-Name-Harvard.svg" class="logo__image only-light" alt="Kempner Institute Computing Handbook - Home"/>
    <script>document.write(`<img src="../../_static/Kempner-logo_Full-Color-Full-Name-Harvard.svg" class="logo__image only-dark" alt="Kempner Institute Computing Handbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Kempner Computing Handbook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">High Performance Computing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../s1_high_performance_computing/kempner_cluster/README.html">1. Kempner AI Cluster</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/kempner_cluster/introduction_and_cluster_basics.html">1.1. Introduction and Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/kempner_cluster/overview_of_kempner_cluster.html">1.2. Overview of Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/kempner_cluster/kempner_policies_for_responsible_use.html">1.3. Cluster Usage Policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/kempner_cluster/accessing_and_navigating_the_cluster.html">1.4. Accessing the Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/kempner_cluster/accessing_gpu_by_fasrc_users.html">1.5. Access by FASRC Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/efficient_use_of_resources/fair_use_and_prioritization_policies.html">1.6. Fairshare Policy</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../s1_high_performance_computing/general_hpc_concepts/README.html">2. Job Submission</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/general_hpc_concepts/understanding_slurm.html">2.1. Understanding SLURM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/general_hpc_concepts/job_submission_basics.html">2.2. Job Submission Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/general_hpc_concepts/array_jobs.html">2.3. Array Jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/general_hpc_concepts/job_dependencies.html">2.4. Job Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/general_hpc_concepts/advanced_slurm_features.html">2.5. Advanced SLURM Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/general_hpc_concepts/open_ondemand.html">2.6. Open OnDemand</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../s1_high_performance_computing/development_and_runtime_envs/README.html">3. Development Environment</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/development_and_runtime_envs/software_module_and_environment_management.html">3.1. Software Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/development_and_runtime_envs/containerization.html">3.2. Containerization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/development_and_runtime_envs/using_vscode_for_remote_development.html">3.3. VSCode for Remote Dev</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/development_and_runtime_envs/using_conda_env.html">3.4. Conda Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/development_and_runtime_envs/handling_dependencies_with_spack.html">3.5. Spack Package Manager</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/development_and_runtime_envs/customizing_bashrc.html">3.6. Shell Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../s1_high_performance_computing/storage_and_data_transfer/README.html">4. Storage and Data Transfer</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/storage_and_data_transfer/understanding_storage_options.html">4.1. Storage Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/storage_and_data_transfer/data_transfer.html">4.2. Data Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s1_high_performance_computing/storage_and_data_transfer/shared_data_repository.html">4.3. Shared Data/Model Repository</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering for Research</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../s2_swe_for_research/collaborative_code_development.html">5. Collaborative Code Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s2_swe_for_research/software_design_principles.html">6. Software Design Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s2_swe_for_research/documentation_and_readibility.html">7. Documentation and Readability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s2_swe_for_research/testing_and_continuous_integration.html">8. Testing and Continues Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s2_swe_for_research/package_development.html">9. Package Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s2_swe_for_research/reproducible_research.html">10. Reproducible Research</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">AI Tools and Workflows</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../s3_ai_workflows/testbed_and_tatm.html">11. Data Discovery and Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s3_ai_workflows/distributed_inference.html">12. Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s3_ai_workflows/nemo_workflow.html">13. NVIDIA NeMo Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s3_ai_workflows/scalable_vision_workflows.html">14. Scalable Vision Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../s3_ai_workflows/get_hf_model.html">15. Hugging Face Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neuro AI Workflows</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../s4_neuro_ai_workflows/spike_sorting.html">16. Spike Sorting</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">AI Scaling and Engineering</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="README.html">17. Scalability</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">17.1. Distributed Computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction_to_parallel_computing.html">17.2. Intro to Parallel Computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu_computing.html">17.3. GPU Computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu_profiling.html">17.4. GPU Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_gpu_computing.html">17.5. Distributed GPU Computing</a></li>
<li class="toctree-l2"><a class="reference internal" href="parallel_io.html">17.6. Parallel I/O</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../efficiency/README.html">18. Efficiency</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../efficiency/ml_scaling_and_efficiency.html">18.1. ML Efficiency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../efficiency/performance_monitoring_and_optimization.html">18.2. Peformance Monitoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../efficiency/efficient_deployment_and_inference.html">18.3. Deployment and Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../efficiency/experiment_management_and_reproducibility.html">18.4. Experiment Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../experiment_management/README.html">19. Experiment Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../experiment_management/logging_and_monitoring.html">19.1. Weights &amp; Biases - Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../experiment_management/wandb_sweeps.html">19.2. Weights &amp; Biases - Sweeps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Security and Compliance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../s6_security_and_compliance/README.html">20. Security and Compliance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Open Source Hub</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../s7_open_source_hub/README.html">21. Open Source Hub</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../s8_support/README.html">22. Support and Troubleshooting</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../s8_support/faq.html">22.1. FAQ</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshops</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../s9_workshops_and_trainings/README.html">23. About Workshops @ Kempner</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../s9_workshops_and_trainings/intro_to_kempner_ai_cluster/intro_to_kempner_ai_cluster.html">23.2. Introduction to the Kempner AI cluster Workshop</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../s9_workshops_and_trainings/intro_to_distributed_computing/intro_to_distributed_computing.html">23.3. Introduction to Distributed Computing Workshop</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/KempnerInstitute/kempner-computing-handbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/KempnerInstitute/kempner-computing-handbook/issues/new?title=Issue%20on%20page%20%2Fs5_ai_scaling_and_engineering/scalability/distributed_computing.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/s5_ai_scaling_and_engineering/scalability/distributed_computing.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Distributed Computing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-one-computer-is-not-enough">17.1.1. When One Computer is Not Enough</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-slurm-handles-multinode-jobs">17.1.2. How Slurm Handles Multinode Jobs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-task">17.1.2.1. What is a “Task”?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#typical-multinode-job-patterns">17.1.3. Typical Multinode Job Patterns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#head-and-worker-nodes">17.1.3.1. Head and Worker Nodes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-frameworks-for-multinode-jobs">17.1.4. Common Frameworks for Multinode Jobs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">17.1.4.1. Pytorch</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytoch-lighting">17.1.4.1.1. Pytoch Lighting</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray">17.1.4.2. Ray</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-ray">17.1.4.2.1. Installing Ray</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ray-on-slurm">17.1.4.2.2. Using Ray on SLURM</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dask">17.1.4.3. Dask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi">17.1.4.4. MPI</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="distributed-computing">
<h1><span class="section-number">17.1. </span>Distributed Computing<a class="headerlink" href="#distributed-computing" title="Link to this heading">#</a></h1>
<section id="when-one-computer-is-not-enough">
<h2><span class="section-number">17.1.1. </span>When One Computer is Not Enough<a class="headerlink" href="#when-one-computer-is-not-enough" title="Link to this heading">#</a></h2>
<p>Often times the computational resources of a single node are not enough to solve a problem in a reasonable amount of time. This can be due to data size or limitations of compute, where more CPUs or GPUs are required than are available on a single node. In these cases, it is necessary to distribute the computation across multiple nodes on a shared network. In HPC and ML contexts, there is also typically shared storage and a scheduler that manages the allocation of resources to ease the work of coordinating distributed jobs. Distributed computing provides a means to massively scale up computation but also introduces a number of complexities and challenges. Distributed computing is a broad topic and an entire field of research in its own right. This chapter will provide an overview of the basic concepts and tools for distributed computing in the context of HPC and ML, with a particular focus on working on a SLURM managed cluster.</p>
</section>
<section id="how-slurm-handles-multinode-jobs">
<h2><span class="section-number">17.1.2. </span>How Slurm Handles Multinode Jobs<a class="headerlink" href="#how-slurm-handles-multinode-jobs" title="Link to this heading">#</a></h2>
<p>To request a multinode job, there a number of <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>/<code class="docutils literal notranslate"><span class="pre">salloc</span></code> flags available to the user. Read more about <a class="reference external" href="https://slurm.schedmd.com/sbatch.html"><code class="docutils literal notranslate"><span class="pre">sbatch</span></code></a> and <a class="reference external" href="https://slurm.schedmd.com/salloc.html"><code class="docutils literal notranslate"><span class="pre">salloc</span></code></a> on SLURM official documentation. When requesting resources it is also important to consider whether the resources
being requested are allocated on a per node or per task basis. The key flags for multinode jobs are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-N</span></code> or <code class="docutils literal notranslate"><span class="pre">--nodes</span></code>: The number of nodes to request for the job. This cannot be greater than the number of tasks requested.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-n</span></code> or <code class="docutils literal notranslate"><span class="pre">--ntasks</span></code>: The number of tasks to request for the job. This can be greater than the number of nodes requested, in which case multiple tasks will be run on each node. This can also be made to vary with the number of nodes using the <code class="docutils literal notranslate"><span class="pre">--ntasks-per-node</span></code> flag.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span></code>: This is the number of CPUs per task. This is a per task resource, and the total number of CPUs requested for the job is <code class="docutils literal notranslate"><span class="pre">--ntasks</span></code> <span class="math notranslate nohighlight">\(\times\)</span> <code class="docutils literal notranslate"><span class="pre">-c</span></code>. If the task per node description is uneven then the total number of CPUs will be different on each node. Note that all allocated CPUs on the job headnode will be available to the batch script and that task resource limitation will only be enforced on tasks initialized with <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mem</span></code>: The amount of memory to request per node. Note that this is per node and not per task. Therefore the memory  allocated on each node will be shared by all tasks</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres</span></code>: This flag can be used to request GPUs or other specialized resources. The resources requested with this flag are per node, and the number of resources requested will be shared by all tasks on the node. When there are multiple tasks on a node trying to divide GPUs, GPU selection will typically be done based on the local rank of the task. Frameworks like Pytorch handle this automatically.</p></li>
</ul>
<p>When a multinode job is submitted, slurm will allocate the requested resources and then run the job script on the head node. The other nodes will be allocated
but will be unused unless a task is started on them by using <code class="docutils literal notranslate"><span class="pre">srun</span></code>. In theory tasks can also be started on other nodes through directly sshing into the node and running the task, but this is not recommended as it bypasses the slurm scheduler and can lead to resource contention and other issues. The head node is the node with a rank of 0, and the other nodes are numbered sequentially. The tasks are also numbered sequentially within each node. The environment variables <code class="docutils literal notranslate"><span class="pre">SLURM_NODEID</span></code> and <code class="docutils literal notranslate"><span class="pre">SLURM_PROCID</span></code> can be used to determine the node and task rank of a given task.</p>
<section id="what-is-a-task">
<h3><span class="section-number">17.1.2.1. </span>What is a “Task”?<a class="headerlink" href="#what-is-a-task" title="Link to this heading">#</a></h3>
<p>A task is a single unit of work that can be executed on a single node wihtin a slurm job. Tasks are initialized by calling <code class="docutils literal notranslate"><span class="pre">srun</span></code> within a slurm allocation. Each task has its resources restricted using Cgroups (in a similar manner  to how slurm controls the resources visible to an entire allocation), and the resource for each task can be specified using many of the same flags available to the user when requesting resources for an entire allocation. Slurm automatically handles the placement of tasks on nodes within the allocation, and the user can specify the number of tasks to be run on each node using the <code class="docutils literal notranslate"><span class="pre">--ntasks-per-node</span></code> flag at job submission time. Slurm also sets environment variables within each task process indicating the Node rank and Task rank wihin the allocation. These can be used to coordinate work between tasks within a single allocation. Most distributed computing frameworks use these environment variables to coordinate work between tasks within a single allocation automatically.</p>
</section>
</section>
<section id="typical-multinode-job-patterns">
<h2><span class="section-number">17.1.3. </span>Typical Multinode Job Patterns<a class="headerlink" href="#typical-multinode-job-patterns" title="Link to this heading">#</a></h2>
<section id="head-and-worker-nodes">
<h3><span class="section-number">17.1.3.1. </span>Head and Worker Nodes<a class="headerlink" href="#head-and-worker-nodes" title="Link to this heading">#</a></h3>
<p>The typical pattern for multinode worker jobs is having a process running on one node that coordinates the work of other processes running on other nodes. This is often referred to as a head and worker pattern. The head node is responsible for coordinating the work of the worker nodes, and the worker nodes are responsible for doing the actual computation. The head node is typically responsible for distributing work to the worker nodes, collecting results from the worker nodes, and aggregating the results into a final result. The worker nodes are typically responsible for doing the actual computation, and they may also be responsible for collecting and sending results back to the head node. The head and worker nodes may communicate with each other using a variety of different communication patterns, typically over the network, but where shared file systems exist we also observe a pattern of writing intermediate results to shared storage.</p>
<p>This pattern can be used both for distributed training jobs or embarassingly parallel tasks that don’t require the level of communication of distributed training. By handling the scheduling of tasks within a single job rather than outsource the scheduling and distribution of small pieces of work to slurm, we can avoid the overhead of scheduling and launching many small jobs and instead use a single job to manage the distribution of work across many nodes and improve the performance of both our job and of the  cluster as a whole. The frameworks described below all support this pattern, and it is the most common pattern for distributed computing on HPC clusters.</p>
</section>
</section>
<section id="common-frameworks-for-multinode-jobs">
<h2><span class="section-number">17.1.4. </span>Common Frameworks for Multinode Jobs<a class="headerlink" href="#common-frameworks-for-multinode-jobs" title="Link to this heading">#</a></h2>
<section id="pytorch">
<h3><span class="section-number">17.1.4.1. </span>Pytorch<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h3>
<p>Pytorch is typically thought of as a library for writing ML training pipelines, but its ecosystem also includes a number of utilities and libraries that
support multinode training.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> package provides a number of tools for coordinating distributed training, including a <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> module that wraps a model and handles the distribution of data and gradients across multiple nodes.</p></li>
<li><p>Pytorch also includes a <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code> package that can be used to spawn multiple processes on a single node, which can be used to create a worker pool for distributed training.</p></li>
<li><p>Pytorch also includes a <code class="docutils literal notranslate"><span class="pre">torch.distributed.rpc</span></code> package that can be used to create distributed RPC services, which can be used to coordinate distributed training or to create distributed parameter servers.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please see the <a class="reference external" href="https://pytorch.org/docs/stable/index.html">pytorch documentation</a> for more information on how to use these packages.</p>
</div>
<section id="pytoch-lighting">
<h4><span class="section-number">17.1.4.1.1. </span>Pytoch Lighting<a class="headerlink" href="#pytoch-lighting" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://lightning.ai/docs/pytorch/stable/">Pytorch Lighting</a> is a highly recommended library, although it is not officially part of the pytorch distribution is. It includes a number of utilities and has built in easy integration with slurm to support multi GPU training. In experiments on the kempner cluster, we have seen pytorch lightning provide near linear scaling as the number of nodes is increased.</p>
<p>Pytorch and Pytorch lightning support multiple communicatoin backends including both MPI and NCCL. This enables users to make backend changes and optimizations while keeping their model definition and training pipeline the same.</p>
</section>
</section>
<section id="ray">
<h3><span class="section-number">17.1.4.2. </span>Ray<a class="headerlink" href="#ray" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://ray.io">Ray</a> is a distributed computation library for python that supports both single and multi-node execution. Ray is designed to be easy to use and provides a number of high-level abstractions for distributed computing, including a task-based API, a distributed actor API, and a distributed data API. Ray also includes a number of utilities for distributed machine learning, including a distributed data loader and a distributed hyperparameter search library.</p>
<p>It serves as the backend for a number of tools in the modern AI/ML ecosystem, including <a class="reference external" href="https://github.com/vllm-project/vllm">vLLM</a>, <a class="reference external" href="https://github.com/mlfoundations/dclm">DCLM</a> and others.</p>
<p>Ray clusters can be run on your laptop as well as on SLURM. The methods of starting the cluster are different, but once the cluster is started the python API is the same, enabling easy development of Ray applications for the cluster on your laptop.</p>
<section id="installing-ray">
<h4><span class="section-number">17.1.4.2.1. </span>Installing Ray<a class="headerlink" href="#installing-ray" title="Link to this heading">#</a></h4>
<p>Ray can be installed using pip. The following command will install Ray and all of its dependencies.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">ray</span><span class="p">[</span><span class="n">default</span><span class="p">]</span>
</pre></div>
</div>
<p>While Ray is available on <code class="docutils literal notranslate"><span class="pre">conda-forge</span></code>, the version available there is maintained by the community and may not be the most up to date. It is recommended to install Ray using pip. See the <a class="reference external" href="https://docs.ray.io/en/latest/ray-overview/installation.html#installing-from-conda-forge">Ray documentation</a> for more information on how to install Ray.</p>
</section>
<section id="using-ray-on-slurm">
<h4><span class="section-number">17.1.4.2.2. </span>Using Ray on SLURM<a class="headerlink" href="#using-ray-on-slurm" title="Link to this heading">#</a></h4>
<p>In order to use Ray on SLURM, you will need to start a Ray cluster and ensure that it is using the proper number of resources. The following script can be used to start a Ray cluster on SLURM. This script will start a Ray head node on the first node in the allocation and then start Ray workers on the rest of the nodes. The script will then start a python script that uses Ray to do distributed computation. Note the use of the SLURM environment variables. By default, Ray will attempt to use all CPUs on a node regardless of the number of tasks requested. This can be controlled by setting the <code class="docutils literal notranslate"><span class="pre">--num-cpus</span></code> flag when starting the Ray cluster. We do this in the following script by setting the number of CPUs to the number of CPUs per task requested by the user. We also set the number of GPUs to the number of GPUs per node requested by the user.</p>
<p>Some specific things to note about the script:</p>
<ul class="simple">
<li><p>The script will start a Ray head node on the first node in the allocation and then start Ray workers on the rest of the nodes.</p></li>
<li><p>The script defines a custom temporary directory for ray to use, based on the user and job id. This is important as Ray will store logs and other temporary files in this directory. If we don’t specify a custom directory, Ray will use the default <code class="docutils literal notranslate"><span class="pre">/tmp/ray</span></code> directory, which can lead to conflicts if multiple users are running Ray jobs on the same node. However this makes it so that <code class="docutils literal notranslate"><span class="pre">ray.init()</span></code> can’t automatically connect to the ray cluster without specifying the address. We use the <code class="docutils literal notranslate"><span class="pre">RAY_ADDRESS</span></code> environment variable to specify the address of the head node without having to modify the script.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#! /bin/bash
#SBATCH --job-name=ray_cluster
#SBATCH -N 2 # number of nodes, this script is adaptable for any number of nodes
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=40 # number of cpus per task, this script can work with any number of cpus
#SBATCH --gres=gpu:4 # number of gpus per node, note that GPUs are not required for Ray
#SBATCH --mem=0 # memory per node, note that this is per node and not per task
#SBATCH --time=0:30:00 # time limit for the job
#SBATCH -p kempner_requeue
#SBATCH --account=&lt;your account&gt;




# Load environment with Ray installed, as well as other dependencies
# Can also run Ray inside singularitiy containers, assuming that all needed dependencies are installed in the container

## PLACEHOLDER LOAD ENV COMMAND ##

# Get number of GPUs on each node
if [[ -z &quot;$SLURM_GPUS_ON_NODE&quot; ]]; then
    export RAY_GPUS=0
else
    export RAY_GPUS=$SLURM_GPUS_ON_NODE
fi

# choose available port on the head node
head_port=`comm -23 &lt;(seq 15000 20000 | sort) &lt;(ss -Htan | awk &#39;{print $4}&#39; | cut -d&#39;:&#39; -f2 | sort -u) | shuf | head -n 1`
nodes=`scontrol show hostnames $SLURM_JOB_NODELIST`
nodes_array=( $nodes )
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w &quot;$head_node&quot; hostname --ip-address)
echo &quot;Head node: $head_node&quot;
echo &quot;Head node ip: $head_node_ip&quot;
echo &quot;Head port: $head_port&quot;
export head_addr=&quot;$head_node_ip:$head_port&quot;
echo &quot;Head address: $head_addr&quot;

echo &quot;Starting Ray head on $head_node&quot;
srun -N 1 -n 1 -w &quot;$head_node&quot; ray start --head --node-ip-address=&quot;$head_node_ip&quot; --temp-dir /tmp/$USER/$SLURM_JOB_ID/ray \
    --port=$head_port --num-cpus $SLURM_CPUS_PER_TASK --num-gpus $RAY_GPUS --min-worker-port 20001 --max-worker-port 30000 --block &amp;

# wait for head node to start
sleep 5

# start ray on the rest of the nodes
worker_num=$((SLURM_NNODES - 1))
for (( i = 1; i &lt;= worker_num; i++ )); do
    node=${nodes_array[$i]}
    echo &quot;Starting Ray worker on $node&quot;
    srun -N 1 -n 1 -w &quot;$node&quot;  ray start --address=&quot;$head_addr&quot; \
        --num-cpus $SLURM_CPUS_PER_TASK --num-gpus $RAY_GPUS --min-worker-port 20001 --max-worker-port 30000 --block &amp;
    sleep 5
done

# wait for all nodes to start
sleep 5

# export RAY_ADDRESS for your script to connect with custom tempdir
export RAY_ADDRESS=&quot;$head_addr&quot;

# Start your script here
python my_ray_script.py
</pre></div>
</div>
<p>Your python script will then be able to call <code class="docutils literal notranslate"><span class="pre">ray.init()</span></code> to connect to the Ray cluster and use the Ray API to do distributed computation. Tasks and actors created
during your script will run across the Ray cluster, with Ray handling scheduling, communication, and placement. Upon completion of the script, the Ray cluster will be shut down and the resources will be released as SLURM terminates the job.</p>
</section>
</section>
<section id="dask">
<h3><span class="section-number">17.1.4.3. </span>Dask<a class="headerlink" href="#dask" title="Link to this heading">#</a></h3>
<p>[TODO]</p>
</section>
<section id="mpi">
<h3><span class="section-number">17.1.4.4. </span>MPI<a class="headerlink" href="#mpi" title="Link to this heading">#</a></h3>
<p>[TODO]</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./s5_ai_scaling_and_engineering/scalability"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="README.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">17. </span>Scalability</p>
      </div>
    </a>
    <a class="right-next"
       href="introduction_to_parallel_computing.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17.2. </span>Intro to Parallel Computing</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-one-computer-is-not-enough">17.1.1. When One Computer is Not Enough</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-slurm-handles-multinode-jobs">17.1.2. How Slurm Handles Multinode Jobs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-task">17.1.2.1. What is a “Task”?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#typical-multinode-job-patterns">17.1.3. Typical Multinode Job Patterns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#head-and-worker-nodes">17.1.3.1. Head and Worker Nodes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-frameworks-for-multinode-jobs">17.1.4. Common Frameworks for Multinode Jobs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">17.1.4.1. Pytorch</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pytoch-lighting">17.1.4.1.1. Pytoch Lighting</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray">17.1.4.2. Ray</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-ray">17.1.4.2.1. Installing Ray</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ray-on-slurm">17.1.4.2.2. Using Ray on SLURM</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dask">17.1.4.3. Dask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi">17.1.4.4. MPI</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kempner Institute
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright  2024, The President and Fellows of Harvard College.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>