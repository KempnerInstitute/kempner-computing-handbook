# About Workshops @ Kempner

Welcome to the Workshops @ Kempner section of our Computing Handbook. This section is dedicated to fostering continuous learning and professional development through a variety of interactive sessions.
Our workshops are designed to equip participants with practical skills and knowledge, encouraging active engagement and collaboration. Whether you are a beginner eager to acquire new competencies or a seasoned professional aiming to enhance your expertise, our training programs offer valuable opportunities to advance your proficiency and contribute effectively to your field.

## List of Workshops


| Workshop Name                              | Description                                                                 | Target Audience                         |
|--------------------------------------------|-----------------------------------------------------------------------------|------------------------------------------|
| {ref}`Introduction to the Kempner AI Cluster <s9_workshops_and_trainings:intro_to_kempner_ai_cluster>`     | Overview of how to access and use the Kempner AI cluster.    | Kempner AI Cluster Users        |
| {ref}`Introduction to Distributed Computing <s9_workshops_and_trainings:intro_to_distributed_computing>`   | Introduction to key concepts in distributed computing.       | Researchers and students with basic SLURM experience and some familiarity with neural networks.  |
| {ref}`Large Language Model Distributed Training <s9_workshops_and_trainings:llm_distributed_training>`   | Reviews parallelization techniques including Distributed Data Parallelism (DDP), Model Parallelism (MP), Tensor Parallelism (TP), Pipeline Parallelism (PP), and Fully Sharded Data Parallelism (FSDP). Provides hands-on examples for each approach.   | Researchers and developers familiar with Python, PyTorch, and LLMs.  |
| {ref}`Building a Transformer from Scratch <s9_workshops_and_trainings:transformer_from_scratch>`   | Provides a practical, interactive way to learn about transformers by building a simple language model.   | Researchers and developers familiar with Python, PyTorch, and foundational machine learning topics.  |
| {ref}`Large Language Model Distributed Inference <s9_workshops_and_trainings:llm_distributed_inference>`   | Provides hands-on training on hosting and running inference for large langauge models that don't fit into a single GPU's memory.   | Researchers and developers who have familiarity with Python, LLMs, and high performance computing, and access to an AI cluster.  |
| {ref}`Spike Sorting on an HPC Cluster <s9_workshops_and_trainings:spike_sorting_on_hpc_cluster>`   | Describes how to implement spike sorting algorithms for neural data on an HPC cluster, using a comprehensive pipeline and interactive examples.   | Researchers and developers who have familiarity with HPC, including slurm batch job submission, and the concept of spike sorting.  Participants should have access to 02 or the FASRC cluster.  |
| {ref}`Optimizing ML Workflows on an AI Cluster <s9_workshops_and_trainings:optimizing_ml_workflows>`   | Demonstrates how to optimize machine learning workflows for efficient, reproducible training on an AI cluster. | Researchers and developers who have familiarity PyTorch and HPC, including slurm batch job submission.  Participants should have access to the FASRC cluster.  |